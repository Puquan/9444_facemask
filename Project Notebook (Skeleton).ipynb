{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cc9da9-ebb2-438c-8da8-34a73b16e4d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FaceMask Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e990bc1-3fd6-4785-936a-d5e633553eb4",
   "metadata": {},
   "source": [
    "`Group Name: Gogogo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755c7b7-e9dc-4aba-a9e9-145127d0ceac",
   "metadata": {},
   "source": [
    "\n",
    "|     Name      | Student ID |\n",
    "| :-----------: | :--------: |\n",
    "|  Puquan Chen  |  z5405329  |\n",
    "| Wenzhen Zhang |  z5282188  |\n",
    "|   Zeran Qiu   |  z5237346  |\n",
    "| Xiaolan Zhang |  z5400028  |\n",
    "|  Haoyu Zang   |  z5326339  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da956912-10aa-46b7-91fa-909b7d0f5074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3c167d3-1290-449a-be41-e5726dce16a1",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c305c-db80-414e-944d-356e29879ac8",
   "metadata": {},
   "source": [
    "## 1.1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b85cc-4338-462f-9a9b-bf255c93853b",
   "metadata": {},
   "source": [
    "COVID-19 has been going on for three years. Although its impact is waning in many areas, we still do not have a good solution to the harm it causes to humans. Therefore the use of facial masks has significantly increased on many occasions. Nevertheless, it is still important to develop social awareness of residents to wear facial masks, since it is common to find people in narrow public spaces without masks. In this case, we need face mask detection to help local authorities better monitor people's compliance with local mask wearing policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b36fc0-71fc-4eff-8ef7-1db832135a87",
   "metadata": {},
   "source": [
    "## 1.2 Motivation\n",
    "As mentioned above, in a climate where COVID-19 is still spreading, we need to ensure that people comply with the mask wearing policy. However, it is very difficult to identify whether people are wearing a mask by human intervention alone, especially in crowded situations such as trains and shopping malls. Therefore, there is a great need for face mask detection to help us quickly and accurately identify whether people are wearing masks or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b35c9-d621-4775-91b2-c4ea5fbbadfb",
   "metadata": {},
   "source": [
    "## 1.3 Purpose/ Project Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3a69e-2768-43d0-9d93-77675a6af80f",
   "metadata": {},
   "source": [
    "\n",
    "In this project, our group applied three existing popular deep learning object detection models which includes `RCNN`, `YOLO` and `SSD`, aim to compare their performance in accuracy, training time and loss rate by using the same data set. Aftering getting all necessary data and details, our group will build our own model based on the existing models mentioned in above and try to analysis the methods which can be used and improve the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d6f9c",
   "metadata": {},
   "source": [
    "## 1.4 Common Techinque / Key Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24b785",
   "metadata": {},
   "source": [
    "**1.Intersection over Union (IoU):** \n",
    "\n",
    "The Intersection over Union (IoU) metric is essentially a method used usually to quantify the percent overlap between the ground truth BBox (Bounding Box) and the prediction BBox. However, in NMS, we find IoU between two predictions BBoxes instead.\n",
    "![jupyter](./notebook_images/iou.png)\n",
    "\n",
    "\n",
    "**2.Anchor box:**\n",
    "\n",
    "Anchor boxes are a set of predefined boxes with scaled sizes and width-height ratios. Anchor box was first adopted in YOLO since YOLOv2. The main idea of using anchor boxes is, instead of generating the entire bounding box, bounding boxes for each grid cell can be created by tuning the initial anchor boxes. And the size and shape of anchor boxes can be set up either with experience or using k-means clusters on the ground truth boxes of the dataset. In YOLOv5, other than setting up the anchor by hand, the system could adaptively calculate the best anchor based on the dataset we use during the training stage.\n",
    "\n",
    "**3.Non-Maximum Suppression (NMS):**\n",
    "\n",
    "NMS is a class of algorithms to select one entity (e.g., bounding boxes) out of many overlapping entities. We can choose the selection criteria to arrive at the desired results. \n",
    "![jupyter](./notebook_images/nms.jpg)\n",
    "\n",
    "The process can be described in 3 main steps:\n",
    "\n",
    "Assume we get a list P of prediction bounding boxes, and each bbox is associated with a predicted confidence score c (For simplicity, we only take one confidence score)\n",
    "\n",
    "1.Select the prediction S with highest class probability and remove it from P and add it to the final prediction list.\n",
    "\n",
    "2.Now compare this prediction S with all the predictions present in P. Calculate the IoU of this prediction S with every other predictions in P. If the IoU is greater than the threshold thresh_iou for any prediction T present in P, remove prediction T from P.\n",
    "\n",
    "3.If there are still predictions left in P, then go to Step 1 again, else return the final prediction list containing the filtered predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3844475-7d00-4afe-8416-a1f5e47ac501",
   "metadata": {},
   "source": [
    "# 2. Data Sources / Preparation of Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685d080-4022-4398-84de-2d07bbd7a186",
   "metadata": {},
   "source": [
    "**All scripts used in this part are stored in the directory** `./tool_scripts` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cb077-1ff1-4e64-a62f-5a0f08bc51f2",
   "metadata": {},
   "source": [
    "## 2.1 Basic Information about The Data \n",
    "\n",
    "Our original face mask dataset contains 6120 images, it comes from https://github.com/AIZOOTech/FaceMaskDetection\n",
    "\n",
    "Nevertheless, the Face Mask Dataset is a combined dataset, the training set made up of 3114 images of Wider Face and 3006 images of masking FAces (MAFA) datasets. It contains normal faces with different lighting, poses, occlusion, and masked face.The test set has 1839 images which includes 780 Wider Face and 1059 MAFA dataset. Here are some samples of original images include in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266dba74-fa12-4802-ab01-8245ceb11018",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 这边泽然补一下sample image，然后看看这部分上下，语句有没有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32ace4-f605-4e0c-8084-6ac4a5b7a00e",
   "metadata": {},
   "source": [
    "Since some format of label files in original dataset are not readable, the pre-processing has been applied to the original dataset. The images in available formats were identified and some invalid dataset was deleted. However, after processing and editing, the number of reliable images reduced dramatically which could affect the following training, our group collected and added extra images to ensure the amount and quality of the whole dataset. Extra datasets refers to the link below:\n",
    "1. https://public.roboflow.com/object-detection/mask-wearing\n",
    "2. https://www.kaggle.com/datasets/andrewmvd/face-mask-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbfecb-4352-4280-af32-24a1ac70f01c",
   "metadata": {},
   "source": [
    "## 2.2 Split Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66033d95-2645-4d19-90da-1277c6797f2c",
   "metadata": {},
   "source": [
    "After integrating three datasets, we try to split the whole datasets into training set, validation set and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221497b-cc70-4787-b527-5d023cce5501",
   "metadata": {},
   "source": [
    "We used [split_dataset.py](./tool_scripts/split_dataset.py) to do the spliting task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c038db-5e74-40f9-b987-8cda5a38af70",
   "metadata": {},
   "source": [
    "The script can be run on dataset with the command below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9081b-cc37-46e8-a6e6-241f5ccb2bd7",
   "metadata": {},
   "source": [
    "`python3 split_dataset.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc64c1-2def-441d-ab82-f14259fca502",
   "metadata": {},
   "source": [
    "Our final face mask dataset contains `18756` training images, `2993` validation images and `2993` testing images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec5e0b-30c9-4778-818e-603547987d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Process Dataset Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12ac7c-9ef6-4ab6-b2ba-10851e1fa660",
   "metadata": {},
   "source": [
    "We tried to train our models based on different frameworks, we used `yolov5` for yolo and `mmdetection` for others, but they need different format of labels, such as `xml`, `txt`, and `json`. Therefore, we created and ran scripts that those label files can be interchangeable\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beaf857-e820-4781-ac9c-5cc47c27f770",
   "metadata": {},
   "source": [
    "The usage of those scripts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c564-8dc6-415b-931a-3faecb326d94",
   "metadata": {},
   "source": [
    "- 1. [formatting_xml.py](./tool_scripts): reformat the `xml` files so that it can be processed by other scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2938ca-c42b-4c0c-9a6c-a34338b0a171",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223e7c9-3e87-466a-ad62-66ba9befda31",
   "metadata": {},
   "source": [
    "- 2. [xml_to_txt.py](./tool_scripts/xml_to_txt.py): convert `xml` format labels to `txt` format labels. The `txt` format is also called yolo format, which only contains the information about object class, object coordinates, height, and width as the image shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f92f3-20bd-4ef0-9927-cab0521afd6b",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70756f56-c48b-4e88-ad4c-ac985eaa7dd1",
   "metadata": {},
   "source": [
    "- 3. [txt_to_xml.py](./tool_scripts/txt_to_xml.py): convert `txt` format labels to `xml` format labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8968713-eee9-4b83-a44a-ea6ae8dc7f06",
   "metadata": {},
   "source": [
    "- 3. [voc2coc.py](./tool_scripts/voc2coc.py): convert `xml` format labels to `json` format labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807cf088-2a6a-46d8-b303-97f00ebc2779",
   "metadata": {},
   "source": [
    "Code reference of this part can be found in [tool_scripts/README.md](./tool_scripts/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb146e9-1143-4215-abe0-ad7e0d97d57a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8208c2d-c319-4c09-9a74-b5a685b444b0",
   "metadata": {},
   "source": [
    "The source codes of each part of experiments are stored in the directory `source_code`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602fd6d8-659f-4a9e-959c-d008d3d663aa",
   "metadata": {},
   "source": [
    "All models were trained and tested on a [Cloud platform](https://www.autodl.com/), using Nvidia RTX 3090. (GPU Memory Size: 24 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715533a-bc44-4d15-b225-2b882ee0e6c8",
   "metadata": {},
   "source": [
    "We tried to stop training until each model became converged, but it was very time-comsuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920f3f9-c0ec-4a25-b2e5-60c6d4ebf150",
   "metadata": {},
   "source": [
    "## 3.1 RCNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ebf4f-20b8-4549-b22f-e74bbca00041",
   "metadata": {},
   "source": [
    "### 3.1.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921bbfa-6446-4dfc-a958-38257ace2e16",
   "metadata": {},
   "source": [
    "Faster R-CNN is an improvement on R-CNN. R-CNN belongs to a two-stage which means first generating a region proposal, and then predicting the classification and location of the target by means of convolutional neural networks. R-CNN uses CNN to replace traditional feature extraction methods in object detection, which can extract better feature information. It also uses a migration learning approach to better improve object detection when the dataset is small at that time. The main reason we choose Faster-RCNN is the feature extraction which RCNN does not have. It maps the region proposals directly to the feature map of the last convolutional layer of the CNN, which reduces the feature extraction time.\n",
    "We choose MMDetection as working framework, MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project.The master branch works with PyTorch 1.5+.#直接复制的#\n",
    "refer:https://github.com/open-mmlab/mmdetection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f696f-efb0-4c9e-84ac-3753cd10a711",
   "metadata": {},
   "source": [
    "### 3.1.2 Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a11e7-209e-453d-b32f-f599eaee58dd",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/7.png\" >\n",
    "refer:https://arxiv.org/abs/1506.01497"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bf676-9d58-4609-8770-2ed8b01b5db4",
   "metadata": {},
   "source": [
    "### 3.1.3 Experiment Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e3c98-5e20-450d-a7c3-7dee20d4249a",
   "metadata": {},
   "source": [
    "The source code of this part is in the directory `source_code/Faster-rcnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06e160-f7ef-4e9f-9ed1-f782ee93eefd",
   "metadata": {},
   "source": [
    "#### Install environment\n",
    "- MMDetection works on Linux, Windows and macOS. It requires Python 3.6+, CUDA 9.2+ and PyTorch 1.5+.\n",
    "1. `conda create --name openmmlab python=3.8 -y`\n",
    "2. `conda activate openmmlab`\n",
    "3. `conda install pytorch=1.8 torchvision cudatoolkit=10.2 -c pytorch`\n",
    "( this step you should change according to your cuda version )\n",
    "4. `pip install -U openmim`\n",
    "5. `mim install mmcv-full`\n",
    "( MMDetection requires mmcv-full>=1.3.17, <1.7.0）\n",
    "6. `cd Faster-rcnn`\n",
    "7. `pip install -r requirements/build.txt`\n",
    "8. `pip install -v -e .  # or \"python setup.py develop\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd215919-576e-4f91-9bcb-8ab1d2fad607",
   "metadata": {},
   "source": [
    "#### Training\n",
    "We set most of hyperparameters as default.\n",
    "The hyperparameters of training can be found in [faster-rcnn_150epoch.log](experiment_result/Faster-rcnn/faster-rcnn_150epoch.log) within the directory `experiment_result/Faster-rcnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74e663-caf8-4c32-9f1f-0c100c278286",
   "metadata": {},
   "source": [
    "We used the command below the run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa04d23-0166-4188-9034-b11a6cd99555",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5778848-a134-4025-a78a-8b2919d1ba8d",
   "metadata": {},
   "source": [
    "The train result is saved at the directory train_mask(when you train, it will create this new directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41154d-e920-47fc-a448-430a01550429",
   "metadata": {},
   "source": [
    "If train is interrupted, we used the command below the continue our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928b5a0-36d2-4688-af4d-7c8151b48a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train.py --resume-from ./train_mask/latest.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab33850-a4f2-4e4c-9116-5b010794aaeb",
   "metadata": {},
   "source": [
    "The training output can be found in this file: [faster-rcnn_result.json](experiment_result/Faster-rcnn/faster-rcnn_result.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a822eb-0d13-4a0d-adc5-e4a90dfa068d",
   "metadata": {},
   "source": [
    "### 3.3.5 Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd301541-a354-4753-8f6f-3d65d3cac2a5",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72563cf3-9f5b-44fd-a865-39cb8c253d6d",
   "metadata": {},
   "source": [
    "The below figure is the confusion matrix of our best model of ssd-mobilenetv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e507457-7619-43d6-a786-c9e49973f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###加一下分析###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3ba22-b292-4efa-99b3-b8e95ecec934",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"250\" height = \"300\" src=\"./experiment_result/Faster-rcnn/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458e275-33ba-4797-86ae-81c25b3da474",
   "metadata": {},
   "source": [
    "#### The Plot of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284c63d-e240-4cdf-864b-a435ae6de010",
   "metadata": {},
   "source": [
    "Based on the the figure below we can see that the model is converged at about 20th epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec3c19-715d-4a62-8acd-a94850a707cf",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./experiment_result/Faster-rcnn/map.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db030ad6-8178-4677-abba-4347fd18a8e2",
   "metadata": {},
   "source": [
    "#### mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead8188-75c3-4159-83e5-cf5c5e43e1b0",
   "metadata": {},
   "source": [
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.383 |  0.151   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce739b-5f78-478f-adfe-c348c462d808",
   "metadata": {},
   "source": [
    "#### inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b66f07-f8e6-4424-ae3e-00bd8386d22c",
   "metadata": {},
   "source": [
    "The inference time(FPS) is evaluate by the scrpit [benchmark.py](source_code/Faster-rcnn/tools/analysis_tools/benchmark.py), the testing results varied sometimes, we chose the best performence as the final result, which was `48.1` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59a234-014c-4d86-8d3c-4084367c0c4f",
   "metadata": {},
   "source": [
    "## 3.2 SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523be16-1908-4f95-97e1-30852a146037",
   "metadata": {},
   "source": [
    "### 3.2.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3f05c-9401-4a90-9d52-c405b008c049",
   "metadata": {},
   "source": [
    "Single-shot Multibox Detector, also known as SSD, uses a fully convolutional approach in the network It is widely used for objects detecting in image and is considered as one of the most excellent models in both speed and accuracy due to the base architecture of VGG-16 Architecture [5], while using SSD, we only need one shot to detect multiple images while R-CNN needs two shots, and that is the reason that SSD is much faster than R-CNN [6]. However, while detecting small-scale objects, SSD is much worse than RCNN since they can only be detected in higher resolution layers and SSD is slightly inaccurate than YOLO because the speed gets interrupted due to the gigantic model. Here is the image of the structure of the SSD [7]. SSD has two main components: multi-scale feature maps and convolutional predictor. Multi-scale feature maps improve the accuracy significantly because it uses multiple layers to detect objects independently, and that is one of the important features for this project. In facial mask detection, all images are not presented in the same scale, in that case, SSD makes it efficient for valid and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285f322-8f73-4caa-a194-fd57b9baf828",
   "metadata": {},
   "source": [
    "### 3.2.2 Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f8736-6514-48cb-8249-18f9c55b47fe",
   "metadata": {},
   "source": [
    "#### 3.2.2.1 ssd model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6531f619-efff-40af-a37d-6e8cd0f0f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###补一下介绍###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c54dc3-f79f-401d-8ea8-525beec92b36",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/ssd.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedd4c9-2ff1-433b-a388-168a23f55164",
   "metadata": {},
   "source": [
    "#### 3.2.2.2 MobileNetV2 model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdda5e-9a88-4967-a77b-d2ed6b4b708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###补一下介绍###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c04e3-026e-4144-916a-22d3cfd22008",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"350\" height = \"300\" src=\"./notebook_images/8.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dc77b-4ec7-4507-8690-8cb540c5e940",
   "metadata": {},
   "source": [
    "### 3.2.3 Experiment Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f07ac2-975e-4af2-8201-76e64482716a",
   "metadata": {},
   "source": [
    "We use MMDetection framework to train and test ssd-mobilenetv2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8fcb7-4ca6-40c5-a448-87e8d76f14bd",
   "metadata": {},
   "source": [
    "The source code of this part is in the directory `source_code/ssd-mobilenetv2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec0ca42-3861-4d66-a72f-39c8f216794c",
   "metadata": {},
   "source": [
    "#### Install environment\n",
    "- MMDetection works on Linux, Windows and macOS. It requires Python 3.6+, CUDA 9.2+ and PyTorch 1.5+.\n",
    "1. `conda create --name openmmlab python=3.8 -y`\n",
    "2. `conda activate openmmlab`\n",
    "3. `conda install pytorch=1.8 torchvision cudatoolkit=10.2 -c pytorch`\n",
    "( this step you should change according to your cuda version )\n",
    "4. `pip install -U openmim`\n",
    "5. `mim install mmcv-full`\n",
    "( MMDetection requires mmcv-full>=1.3.17, <1.7.0）\n",
    "6. `cd ssd-mobilenetv2`\n",
    "7. `pip install -r requirements/build.txt`\n",
    "8. `pip install -v -e .  # or \"python setup.py develop\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340a702-fff7-49bb-818a-4832b25340c7",
   "metadata": {},
   "source": [
    "#### Training\n",
    "We set most of hyperparameters as default.\n",
    "The hyperparameters of training can be found in [ssd_300epoch.log](experiment_result/ssd/ssd_300epoch.log) within the directory `experiment_result/ssd-mobilenetv2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e1e08-1109-4a93-80bc-7aa66c65fc60",
   "metadata": {},
   "source": [
    "We used the command below the run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15335885-fcb4-4540-85b9-0b161df633cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418a829-7cf3-4b5b-a133-f07e85425ac6",
   "metadata": {},
   "source": [
    "The train result is saved at the directory train_mask(when you train, it will create this new directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ed6a1-c07b-4308-9d0e-cd95c44bfb26",
   "metadata": {},
   "source": [
    "If train is interrupted, we used the command below the continue our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ee74b-1a5d-4c22-9f1a-321b62160d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train.py --resume-from ./train_mask/latest.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc802db-3503-4713-8560-30c734d10536",
   "metadata": {},
   "source": [
    "The training output can be found in this file: [ssd_result.json](experiment_result/ssd-mobilenetv2/ssd_result.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24ca06-588e-442c-b19d-16b7093033b6",
   "metadata": {},
   "source": [
    "### 3.3.5 Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4b180-0437-4236-af5d-4a3bf95c7edd",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd52db-e7bc-44ab-a062-18f1586aee4a",
   "metadata": {},
   "source": [
    "The below figure is the confusion matrix of our best model of ssd-mobilenetv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663ffc26-416b-4375-98fd-2a305cb7b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###加一下分析这个混淆矩阵###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf427d8-7a4e-429f-8f1d-0e41e55e8794",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"250\" height = \"300\" src=\"experiment_result/ssd-mobilenetv2/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1ca2f-30ef-42db-b2de-f14954eaaec5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The Plot of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210093a3-0c74-419c-8143-db259a88f070",
   "metadata": {},
   "source": [
    "Based on the the figure below we can see that the model is converged at about 200th epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b490b2-4ee5-459d-94a4-f5f990cf68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###map图###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91d0e9-9348-4f84-89ed-24bb415ff69a",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982096a9-fbe6-457e-b4d9-4484e7c5b75a",
   "metadata": {},
   "source": [
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.69 |  0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a3b50-9061-4c2d-a935-a6fbd4c020fc",
   "metadata": {},
   "source": [
    "#### Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8612a-876f-4bd3-9982-895966fab316",
   "metadata": {},
   "source": [
    "The inference time(FPS) is evaluate by the scrpit [benchmark.py](source_code/ssd-mobilenetv2/tools/analysis_tools/benchmark.py), the testing results varied sometimes, we chose the best performence as the final result, which was `85.6` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4a138-c6aa-4c07-a528-996f9115943f",
   "metadata": {},
   "source": [
    "## 3.3 Yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620fd58-edc6-4309-9296-6c3013d01900",
   "metadata": {},
   "source": [
    "### 3.3.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b555a-8c31-4c30-9e7b-d64dcb57adfe",
   "metadata": {},
   "source": [
    "YOLO is a state-of-the-art object detection system. It is one of the most popular algorithms for real-time object detection. YOLO(You Only Look Once) is a `single-stage detector`, which is the same as SSD but is different from RCNN.\n",
    "\n",
    "Since the first debut of YOLOv1 in 2016, YOLO family has been well developed and updated to YOLOv7 although Redmond, the author of YOLOv1, stopped working on YOLO after YOLOv3. For this project, our team chose to use YOLOv5, which was released in 2020, to perform facial mask detection. And instead of a single model, YOLOv5 is a family of compound-scaled object detection models[3], and `YOLOv5s` is used in our project.\n",
    "\n",
    "YOLOv5 comes with various versions, each having its own\n",
    "unique characteristic. These versions being:\n",
    "1. yolov5-n - The nano version\n",
    "2. yolov5-s - The small version\n",
    "3. yolov5-m - The medium version\n",
    "4. yolov5-l – The large version\n",
    "5. yolov5-x - The extra-large version\n",
    "\n",
    "The performance analysis of above models is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8e467-9ea9-4fd4-ba33-24717bacdbb4",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/4.png\" >\n",
    "\n",
    "\n",
    "refer:https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18537a6-2245-4fec-9e98-290cbcf7063c",
   "metadata": {},
   "source": [
    "### 3.3.2 Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d13555-22b4-43c8-a1c7-12c0dbfb87ea",
   "metadata": {},
   "source": [
    "The YOLOv5 network consists of three main parts. \n",
    "1. Backbone - A CNN layer aggregate image features at\n",
    "different scales.\n",
    "2. Neck – Set of layers to combine image features and pass\n",
    "them forward to prediction.\n",
    "3. Head - Takes features from the neck and performs\n",
    "localization and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c86002-99a8-4e1a-90f6-74801961f519",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/5.png\" >\n",
    "ref: https://www.researchgate.net/figure/The-network-architecture-of-Yolov5-It-consists-of-three-parts-1-Backbone-CSPDarknet_fig1_349299852"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290e1dc-8de7-4d80-8e7f-a8584491f9c9",
   "metadata": {},
   "source": [
    "### 3.3.3 Experiment Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8e3ca-b033-45b8-a38a-0e1c1abb0d8f",
   "metadata": {},
   "source": [
    "The source code of this part is in the directory `source_code/yolov5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ec4aa-d468-4efc-b65c-b201271a5d06",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Install environment\n",
    "1. `cd source_code/yolov5`\n",
    "2. `conda create -n yolov5 python=3.8`\n",
    "3. `conda activate yolov5`\n",
    "4. `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0e82f-736e-417d-b177-14d23114dcb8",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55e0dd-7205-4185-af78-49e567671e24",
   "metadata": {},
   "source": [
    "We set most of hyperparameters as default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967d43c-4e18-4481-b3a8-a2ada85cfcd8",
   "metadata": {},
   "source": [
    "The hyperparameters of training can be found in [opt.yaml](experiment_result/yolov5s/640_5s_300epoch/opt.yaml) within the directory `experiment_result/yolov5s/640_5s_300epoch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335daace-3245-4cc7-a495-e71d37a0e504",
   "metadata": {},
   "source": [
    "We used the command below the run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b689e5-1153-45ed-92a2-5211cad41e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --epochs 300 --batch 64 --data './datasets/data.yaml' --cfg 'yolov5s.yaml' --weights '' --project 'facemask_640_yolo5s' --name '640_5s_300epoch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57147cf-8aec-4b87-9649-364f025dc3d9",
   "metadata": {},
   "source": [
    "The training output can be found in this file: [train_record.ipynb](source_code/yolov5/train_record.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8585c07-de87-4886-a3c2-40319e02abf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3.5 Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e039e-8799-4ca8-9d89-faa7ae2eea1d",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483011f3-b7b1-42b6-b93a-beee13c70359",
   "metadata": {},
   "source": [
    "The below figure is the confusion matrix of our best model of yolov5s. Based on this graph, we can see that the background can be misdetected as face or mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21011d94-473c-4287-ac98-25a7bb969d9e",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"experiment_result/yolov5s/640_5s_300epoch/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bb4a1-d1d3-4b1b-85e6-cfba91ff2ef7",
   "metadata": {},
   "source": [
    "#### The Plot of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f9b45-2639-4480-95b8-5d41c828bad0",
   "metadata": {},
   "source": [
    "Based on the the figure below we can see that the model is converged at about 200th epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be558be-20b8-4f55-9ffa-a3db0098b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 图, 就画一个map的图，x轴的epoch需要显示的比较明显一些，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3ddca-ccd8-485e-ae9e-f3e30d659816",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e9902-5654-40f5-8f21-e137b2641acf",
   "metadata": {},
   "source": [
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.887 |  0.504   |\n",
    "| Face  | 0.931 |  0.557   |\n",
    "| Mask  | 0.843 |  0.451   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a18ff-20f9-49bd-9e95-28446aeca67b",
   "metadata": {},
   "source": [
    "We can see that the mask class much lower map than the face class, which matches the result of confusion, because the mask class are easier to be misdected as background than the face class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc10d90-9db9-4052-a170-1e18bf724f39",
   "metadata": {},
   "source": [
    "#### Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbee02-1145-4344-9924-01f83cf2ff85",
   "metadata": {},
   "source": [
    "The inference time(FPS) is evaluate by the scrpit [val.py](source_code/yolov5/val.py), the testing results varied sometimes, we chose the best performence as the final result, which was `92.5` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7da38-fb52-45d0-b66a-c5b34cbf4152",
   "metadata": {},
   "source": [
    "## 3.4 Yolov5-MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d48af-7bf2-45d8-b8db-c56f9ee85704",
   "metadata": {},
   "source": [
    "### 3.4.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ed7e8-2028-434b-93c1-aaf40a0b8a37",
   "metadata": {},
   "source": [
    "We were inspired by the method of ssd-mobilenetV2, it integrates the SSD algorithm and MobileNetV2 networks. Therefore we decided to integrate yolov5 with moblieNet.\n",
    "\n",
    "The mobileNet has three versions, V1, V2 and V3. Its first version (mobileNetV1) has a depthwise separable convolution, which reduces the model complexity and computational cost. The key methmod used in the second version is called inverted residual structure, resulting in keeping the low computational cost and reaching relative higher accuracy compare with mobileNetV1. For V3, it combines the self-attention mechanism, and replaces relu activation with h-swish activation.[1]这里加reference，mobilenet的论文\n",
    "\n",
    "\n",
    "\n",
    "For comparision, we choose mobileNetV2, which is the same as the SSD model used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf38088-4b95-440e-bc88-a2a62b976e06",
   "metadata": {},
   "source": [
    "### 3.4.2 Custom Layers Modification in Yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a76fe-12d1-466f-9c53-10982dd6c258",
   "metadata": {},
   "source": [
    "As the part 3.3.3 introduced, yolov5 has three main parts, we only need to modify the backbone of the yolov5 model, which is used for extracting and processing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1aa3a5-b400-47da-8e9d-2d3a911959f0",
   "metadata": {},
   "source": [
    "ref: \n",
    "1. https://blog.csdn.net/weixin_42182534/article/details/123418604?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123418604-blog-114492726.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123418604-blog-114492726.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=2\n",
    "\n",
    "2. https://github.com/shaoshengsong/YOLOv5-ShuffleNetV2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257b3d5-4117-4956-9dd1-543d7424c2a4",
   "metadata": {},
   "source": [
    "To change the architecture, we need to do:\n",
    "\n",
    "1. add inverted residual structure in the yolov5 model module, see the file [common.py](source_code/yolov5-MobileNetV2/models/common.py), line no.876\n",
    "2. modify the model configuration file of original model, the key concept is to change the original CNN layers into invertedResidual layers, connecting with their corresponding head layers. The final configuration file can be found in the file [yolov5-moblienetv2.yaml](source_code/yolov5-MobileNetV2/models/yolov5-moblienetv2.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87ab49-d07a-44c8-a032-6222937dc7d3",
   "metadata": {},
   "source": [
    "The comparison of model complexity for Yolov5-MobileNetV2 and Yolov5s is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89223856-c9ba-4f8c-aee2-a6bb3ad5a942",
   "metadata": {},
   "source": [
    "|       Model        | Number of Parameters | Number of Layers |\n",
    "| :----------------: | :------------------: | :--------------: |\n",
    "|      Yolov5s       |       7015519        |       157        |\n",
    "| Yolov5-MobileNetV2 |       2916063        |       276        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67273ea9-659d-4739-bff8-47e18d0193a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f44b704c-aabd-46e0-87de-9d3596913341",
   "metadata": {},
   "source": [
    "### 3.4.3 Experiment Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae14a60-db9c-46cf-8a9c-c95ab87d1e06",
   "metadata": {},
   "source": [
    "The traning process and hyperparameter setting were the same as last experiment.\n",
    "\n",
    "The training output can be found in this file: [train_record.ipynb](source_code/yolov5-MobileNetV2/train_record.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590913f5-ec76-41f4-ab2e-e5a881991690",
   "metadata": {},
   "source": [
    "### 3.4.4 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80d9ff-a2ec-4ef6-a35c-79dcd39c85e0",
   "metadata": {},
   "source": [
    "#### Confusion Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd21236-5007-4566-9c7b-e0159ce29e73",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"450\" height = \"300\" src=\"experiment_result/yolov5-MobileNetV2/640_300epoch/confusion_matrix.png\" >\n",
    "\n",
    "<img style=\"float: left;\" width = \"450\" height = \"300\" src=\"experiment_result/yolov5s/640_5s_300epoch/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5127b-36e1-4c0f-9ab2-f2300e7bb91b",
   "metadata": {},
   "source": [
    "According to the matrics above, the left is for the Yolov5-MoblieNetV2, the right is for the Yolov5s, we can found that the performance of accuracy slightly decreased. The characteristic of misdetection of mask,face and background remained as yolov5s model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fb258-11bb-4393-849e-b591ab3bb9c9",
   "metadata": {},
   "source": [
    "#### The Plot of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc87f6-ecbc-40e0-b985-0f1a0d4d9c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7f5c873-c51b-4617-b248-cb55698f6c9d",
   "metadata": {},
   "source": [
    "Based on the the figure below we can see that the model is converged at about 100th epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac510f-a60d-400e-821a-c183e220aceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9908160e-1309-4d51-bff1-9d5329612cd4",
   "metadata": {},
   "source": [
    "#### Map\n",
    "\n",
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.871 |  0.478   |\n",
    "| Face  | 0.919 |  0.533   |\n",
    "| Mask  | 0.823 |  0.423   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b41dd-0205-4c53-82c6-46df5c7b3cf9",
   "metadata": {},
   "source": [
    "#### Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8b53e-d1d9-46fb-ae4f-dcdd98f6435a",
   "metadata": {},
   "source": [
    "The testing results varied sometimes, we chose the best performence as the final result, which was `95.2` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3859dd-5897-440d-b1bc-5c27c6878a4d",
   "metadata": {},
   "source": [
    "# 4. Comparision/ Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a78141-2fcc-493b-8516-841ff32801f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
