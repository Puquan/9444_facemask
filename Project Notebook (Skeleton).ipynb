{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cc9da9-ebb2-438c-8da8-34a73b16e4d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FaceMask Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e990bc1-3fd6-4785-936a-d5e633553eb4",
   "metadata": {},
   "source": [
    "`Group Name: Gogogo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755c7b7-e9dc-4aba-a9e9-145127d0ceac",
   "metadata": {},
   "source": [
    "\n",
    "|     Name      | Student ID |\n",
    "| :-----------: | :--------: |\n",
    "|  Puquan Chen  |  z5405329  |\n",
    "| Wenzhen Zhang |  z5282188  |\n",
    "|   Zeran Qiu   |  z5237346  |\n",
    "| Xiaolan Zhang |  z5400028  |\n",
    "|  Haoyu Zang   |  z5326339  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c167d3-1290-449a-be41-e5726dce16a1",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c305c-db80-414e-944d-356e29879ac8",
   "metadata": {},
   "source": [
    "## 1.1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b85cc-4338-462f-9a9b-bf255c93853b",
   "metadata": {},
   "source": [
    "COVID-19 has been going on for three years. Although its impact is waning in many areas, we still do not have a good solution to the harm it causes to humans. Therefore the use of facial masks has significantly increased on many occasions. Nevertheless, it is still important to develop social awareness of residents to wear facial masks, since it is common to find people in narrow public spaces without masks. In this case, we need face mask detection to help local authorities better monitor people's compliance with local mask wearing policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b36fc0-71fc-4eff-8ef7-1db832135a87",
   "metadata": {},
   "source": [
    "## 1.2 Motivation\n",
    "As mentioned above, in a climate where COVID-19 is still spreading, we need to ensure that people comply with the mask wearing policy. However, it is very difficult to identify whether people are wearing a mask by human intervention alone, especially in crowded situations such as trains and shopping malls. Therefore, there is a great need for face mask detection to help us quickly and accurately identify whether people are wearing masks or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b35c9-d621-4775-91b2-c4ea5fbbadfb",
   "metadata": {},
   "source": [
    "## 1.3 Purpose/ Project Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3a69e-2768-43d0-9d93-77675a6af80f",
   "metadata": {},
   "source": [
    "In this project, our group aims to develop a real-time facial mask detection system. We applied three existing popular deep learning object detection models which include Faster R-CNN, YOLO and SSD, aiming to compare their performance by using the same dataset. Our group uses MMDetection and YOLOv5 as the framework while training our data. After getting all the necessary results and details, our group decided to replace the backbone of YOLOv5 and SSD to mobilenetv2 in the innovation part, which may help to improve the performance of the network and models in terms of speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3844475-7d00-4afe-8416-a1f5e47ac501",
   "metadata": {},
   "source": [
    "# 2. Data Sources / Preparation of Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685d080-4022-4398-84de-2d07bbd7a186",
   "metadata": {},
   "source": [
    "All scripts used in this part are stored in the directory `./tool_scripts` \n",
    "\n",
    "Link for Dataset used: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cb077-1ff1-4e64-a62f-5a0f08bc51f2",
   "metadata": {},
   "source": [
    "## 2.1 Basic Information about The Data \n",
    "\n",
    "Our original face mask dataset contains 6120 images, it comes from https://github.com/AIZOOTech/FaceMaskDetection\n",
    "\n",
    "Nevertheless, the Face Mask Dataset is a combined dataset, the training set made up of 3114 images of Wider Face and 3006 images of masking FAces (MAFA) datasets. It contains normal faces with different lighting, poses, occlusion, and masked face.The test set has 1839 images which includes 780 Wider Face and 1059 MAFA dataset. Here are some samples of original images include in dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b887b95",
   "metadata": {},
   "source": [
    "![jupyter](./notebook_images/dataset_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32ace4-f605-4e0c-8084-6ac4a5b7a00e",
   "metadata": {},
   "source": [
    "Since some format of label files in original dataset are not readable, the pre-processing has been applied to the original dataset. The images in available formats were identified and some invalid dataset was deleted. However, after processing and editing, the number of reliable images reduced dramatically which could affect the following training, our group collected and added extra images to ensure the amount and quality of the whole dataset. Extra datasets refers to the link below:\n",
    "1. https://public.roboflow.com/object-detection/mask-wearing\n",
    "2. https://www.kaggle.com/datasets/andrewmvd/face-mask-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbfecb-4352-4280-af32-24a1ac70f01c",
   "metadata": {},
   "source": [
    "## 2.2 Split Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66033d95-2645-4d19-90da-1277c6797f2c",
   "metadata": {},
   "source": [
    "After integrating three datasets, we try to split the whole datasets into training set, validation set and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221497b-cc70-4787-b527-5d023cce5501",
   "metadata": {},
   "source": [
    "We used [split_dataset.py](./tool_scripts/split_dataset.py) to do the spliting task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c038db-5e74-40f9-b987-8cda5a38af70",
   "metadata": {},
   "source": [
    "The script can be run on dataset with the command below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9081b-cc37-46e8-a6e6-241f5ccb2bd7",
   "metadata": {},
   "source": [
    "`python3 split_dataset.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc64c1-2def-441d-ab82-f14259fca502",
   "metadata": {},
   "source": [
    "Our final face mask dataset contains `18756` training images, `2993` validation images and `2993` testing images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec5e0b-30c9-4778-818e-603547987d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Process Dataset Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12ac7c-9ef6-4ab6-b2ba-10851e1fa660",
   "metadata": {},
   "source": [
    "We tried to train our models based on different frameworks, we used `yolov5` for yolo and `mmdetection` for others, but they need different format of labels, such as `xml`, `txt`, and `json`. Therefore, we created and ran scripts that those label files can be interchangeable\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beaf857-e820-4781-ac9c-5cc47c27f770",
   "metadata": {},
   "source": [
    "The usage of those scripts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c564-8dc6-415b-931a-3faecb326d94",
   "metadata": {},
   "source": [
    "- 1. [formatting_xml.py](./tool_scripts): reformat the `xml` files so that it can be processed by other scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2938ca-c42b-4c0c-9a6c-a34338b0a171",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223e7c9-3e87-466a-ad62-66ba9befda31",
   "metadata": {},
   "source": [
    "- 2. [xml_to_txt.py](./tool_scripts/xml_to_txt.py): convert `xml` format labels to `txt` format labels. The `txt` format is also called yolo format, which only contains the information about object class, object coordinates, height, and width as the image shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f92f3-20bd-4ef0-9927-cab0521afd6b",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70756f56-c48b-4e88-ad4c-ac985eaa7dd1",
   "metadata": {},
   "source": [
    "- 3. [txt_to_xml.py](./tool_scripts/txt_to_xml.py): convert `txt` format labels to `xml` format labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8968713-eee9-4b83-a44a-ea6ae8dc7f06",
   "metadata": {},
   "source": [
    "- 4. [voc2coc.py](./tool_scripts/voc2coc.py): convert `xml` format labels to `json` format labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807cf088-2a6a-46d8-b303-97f00ebc2779",
   "metadata": {},
   "source": [
    "Code reference of this part can be found in [tool_scripts/README.md](./tool_scripts/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb146e9-1143-4215-abe0-ad7e0d97d57a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8208c2d-c319-4c09-9a74-b5a685b444b0",
   "metadata": {},
   "source": [
    "The source code of the experiments are stored in the directory `source_code`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602fd6d8-659f-4a9e-959c-d008d3d663aa",
   "metadata": {},
   "source": [
    "### Experiment platform: \n",
    "\n",
    "All models were trained and tested on a [Cloud platform](https://www.autodl.com/) with following configuration:\n",
    "\n",
    "    GPU: NVidia RTX 3090 24GB\n",
    "    CPU: Intel Xeon Platinum 8255C @2.50GHz\n",
    "    OS: Windows 10 and Linux\n",
    "    Platform: Jupyter Notebook (Python 3.8)\n",
    "    Framework: MMdetection (Faster R-CNN and SSD-moblienetv2), YOLOv5 (YOLOv5s and YOLOv5s-mobilenetv2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd66dc6",
   "metadata": {},
   "source": [
    "### Common Techinque / Key Terminology:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24b785",
   "metadata": {},
   "source": [
    "**Intersection over Union (IoU):** \n",
    "\n",
    "The Intersection over Union (IoU) metric is essentially a method used usually to quantify the percent overlap between the ground truth BBox (Bounding Box) and the prediction BBox.[1] In NMS, we find IoU between two predictions BBoxes instead.\n",
    "![jupyter](./notebook_images/iou.png)\n",
    "\n",
    "\n",
    "**Anchor box:**\n",
    "\n",
    "Anchor boxes are a set of predefined boxes with scaled sizes and width-height ratios. The main idea of using anchor boxes is, instead of generating the entire bounding box, bounding boxes for each grid cell can be created by tuning the initial anchor boxes. And the size and shape of anchor boxes can be set up either with experience or using k-means clusters on the ground truth boxes of the dataset. In YOLOv5, other than setting up the anchor by hand, the system could adaptively calculate the best anchor based on the dataset we use during the training stage.\n",
    "\n",
    "**Non-Maximum Suppression (NMS):**\n",
    "\n",
    "NMS is a class of algorithms to select one entity (e.g., bounding boxes) out of many overlapping entities. We can choose the selection criteria to arrive at the desired results. [1]\n",
    "![jupyter](./notebook_images/nms.jpg)\n",
    "\n",
    "The process can be described in 3 main steps:\n",
    "\n",
    "Assume we get a list P of prediction bounding boxes, and each bbox is associated with a predicted confidence score c.\n",
    "\n",
    "1.Select the prediction S with highest class probability and remove it from P and add it to the final prediction list.\n",
    "\n",
    "2.Now compare this prediction S with all the predictions present in P. Calculate the IoU of this prediction S with every other predictions in P. If the IoU is greater than the threshold thresh_iou for any prediction T present in P, remove prediction T from P.\n",
    "\n",
    "3.If there are still predictions left in P, then go to Step 1 again, else return the final prediction list containing the filtered predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920f3f9-c0ec-4a25-b2e5-60c6d4ebf150",
   "metadata": {},
   "source": [
    "## 3.1 Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ebf4f-20b8-4549-b22f-e74bbca00041",
   "metadata": {},
   "source": [
    "### 3.1.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921bbfa-6446-4dfc-a958-38257ace2e16",
   "metadata": {},
   "source": [
    "**R-CNN** (Regions with CNN features)[2] is a new idea born at a time when the traditional idea of object detection has reached a bottleneck. The detection accuracy of traditional object detection algorithms is only about 30%, but the detection accuracy of R-CNN can reach more than 50%, which greatly improves the detection accuracy of object detection. \n",
    "\n",
    "![jupyter](./notebook_images/rcnn.png)\n",
    "\n",
    "R-CNN has two key points: \n",
    "1. It uses convolutional neural networks to replace traditional feature extraction methods (e.g. HOG) in object detection, which can extract better feature information. \n",
    "2. R-CNN uses a migration learning approach to better improve object detection when the object detection dataset is small at the time. \n",
    "\n",
    "The process of R-CNN can be divided into several main steps: \n",
    "1. Generate 1k to 2k candidate regions for the input image using the Selective Search method.\n",
    "2. Feature extraction for each candidate region using a deep neural network. The R-CNN then feeds the feature vectors obtained in the second step into the SVM classifier for each class to determine which class the feature vectors belong to.\n",
    "3. The position of the candidate box is adjusted using the regressor. The following figure shows the exact flow of R-CNN in more detail.\n",
    "\n",
    "\n",
    "**Fast R-CNN**[3] is an improvement on R-CNN and they both use VGG16 as the backbone of the network. Fast R-CNN has faster training time and test inference time and higher accuracy compared to R-CNN. \n",
    "![jupyter](./notebook_images/fastrcnn.png)\n",
    "\n",
    "The algorithm of Fast R-CNN can be divided into three main steps: \n",
    "1. Generate about 2,000 candidate regions for the image using selective search; \n",
    "2. Input the image into the network to obtain the corresponding feature map, and project the candidate regions generated in the previous step onto the feature map to obtain the corresponding feature matrix. \n",
    "3. Each feature matrix is scaled to a feature map of the same size using a region of interest (ROI) pooling layer. Afterwards, the scaled feature maps are flattened and the final prediction is obtained through a series of fully connected layers. The algorithm flow of Fast R-CNN is shown in the following diagram.\n",
    "\n",
    "\n",
    "**Faster R-CNN**[4] is an improvement on Fast R-CNN. It is a two-stage detector, meaning that firstly it would generate a region proposal, and then predict the classification and location of the target by means of convolutional neural networks. The Faster R-CNN is identical to the Fast R-CNN in some parts, except that the Faster R-CNN uses specialized region proposal networks (RPN) instead of a selective search algorithm. Faster R-CNN further improves inference speed and accuracy on top of Fast R-CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f696f-efb0-4c9e-84ac-3753cd10a711",
   "metadata": {},
   "source": [
    "### 3.1.2 Faster R-CNN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a11e7-209e-453d-b32f-f599eaee58dd",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"300\" height = \"150\" src=\"./notebook_images/7.png\" >\n",
    "refer:https://arxiv.org/abs/1506.01497 [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bf676-9d58-4609-8770-2ed8b01b5db4",
   "metadata": {},
   "source": [
    "### 3.1.3 Experiment Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e3c98-5e20-450d-a7c3-7dee20d4249a",
   "metadata": {},
   "source": [
    "We choose MMDetection as working framework, MMDetection is an open source object detection toolbox based on PyTorch.\n",
    "refer:https://github.com/open-mmlab/mmdetection\n",
    "\n",
    "The source code of this part is in the directory `source_code/Faster-rcnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06e160-f7ef-4e9f-9ed1-f782ee93eefd",
   "metadata": {},
   "source": [
    "#### Install environment:\n",
    "- MMDetection works on Linux, Windows and macOS. It requires Python 3.6+, CUDA 9.2+ and PyTorch 1.5+.\n",
    "1. `conda create --name openmmlab python=3.8 -y`\n",
    "2. `conda activate openmmlab`\n",
    "3. `conda install pytorch=1.8 torchvision cudatoolkit=10.2 -c pytorch`\n",
    "( this step you should change according to your cuda version )\n",
    "4. `pip install -U openmim`\n",
    "5. `mim install mmcv-full`\n",
    "( MMDetection requires mmcv-full>=1.3.17, <1.7.0）\n",
    "6. `cd Faster-rcnn`\n",
    "7. `pip install -r requirements/build.txt`\n",
    "8. `pip install -v -e .  # or \"python setup.py develop\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd215919-576e-4f91-9bcb-8ab1d2fad607",
   "metadata": {},
   "source": [
    "#### Training:\n",
    "We set most of hyperparameters as default.\n",
    "The hyperparameters of training can be found in [faster-rcnn_150epoch.log](experiment_result/Faster-rcnn/faster-rcnn_150epoch.log) within the directory `experiment_result/Faster-rcnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74e663-caf8-4c32-9f1f-0c100c278286",
   "metadata": {},
   "source": [
    "We used the command below the run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa04d23-0166-4188-9034-b11a6cd99555",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5778848-a134-4025-a78a-8b2919d1ba8d",
   "metadata": {},
   "source": [
    "The train result is saved at the directory train_mask(when you train, it will create this new directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41154d-e920-47fc-a448-430a01550429",
   "metadata": {},
   "source": [
    "If train is interrupted, we used the command below the continue our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928b5a0-36d2-4688-af4d-7c8151b48a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train.py --resume-from ./train_mask/latest.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab33850-a4f2-4e4c-9116-5b010794aaeb",
   "metadata": {},
   "source": [
    "The training output can be found in this file: [faster-rcnn_result.json](experiment_result/Faster-rcnn/faster-rcnn_result.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a822eb-0d13-4a0d-adc5-e4a90dfa068d",
   "metadata": {},
   "source": [
    "### 3.1.4 Results of Faster R-CNN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd301541-a354-4753-8f6f-3d65d3cac2a5",
   "metadata": {},
   "source": [
    "#### Confusion Matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72563cf3-9f5b-44fd-a865-39cb8c253d6d",
   "metadata": {},
   "source": [
    "The below figure is the confusion matrix of our best model of ssd-mobilenetv2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e507457-7619-43d6-a786-c9e49973f8ec",
   "metadata": {},
   "source": [
    "In the following figure e represents the mask, k represents the face and d represents the background. x-axis is the predicted value and y-axis is the true value. Overall the object detection of the Faster R-CNN is not very good, and there is a high probability that the recognition of faces and masks will be recognized as background. And the background part will also be recognized as a mask or a face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3ba22-b292-4efa-99b3-b8e95ecec934",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"250\" height = \"300\" src=\"./experiment_result/Faster-rcnn/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458e275-33ba-4797-86ae-81c25b3da474",
   "metadata": {},
   "source": [
    "#### The plot of training for Faster R-CNN model:\n",
    "Based on the the figure below we can see that the model is converged at about 20th epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec3c19-715d-4a62-8acd-a94850a707cf",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./experiment_result/Faster-rcnn/map.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db030ad6-8178-4677-abba-4347fd18a8e2",
   "metadata": {},
   "source": [
    "#### mAP of Faster R-CNN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead8188-75c3-4159-83e5-cf5c5e43e1b0",
   "metadata": {},
   "source": [
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.383 |  0.151   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce739b-5f78-478f-adfe-c348c462d808",
   "metadata": {},
   "source": [
    "#### The inference time of Faster R-CNN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b66f07-f8e6-4424-ae3e-00bd8386d22c",
   "metadata": {},
   "source": [
    "The inference time(FPS) is evaluate by the scrpit [benchmark.py](source_code/Faster-rcnn/tools/analysis_tools/benchmark.py), the testing results varied sometimes, we chose the best performence as the final result, which was `48.1` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59a234-014c-4d86-8d3c-4084367c0c4f",
   "metadata": {},
   "source": [
    "## 3.2 SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523be16-1908-4f95-97e1-30852a146037",
   "metadata": {},
   "source": [
    "### 3.2.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3f05c-9401-4a90-9d52-c405b008c049",
   "metadata": {},
   "source": [
    "Single-shot Multibox Detector, also known as SSD, uses a fully convolutional approach in the network It is widely used for objects detecting in image and is considered as one of the most excellent models in both speed and accuracy due to the base architecture of VGG-16 Architecture [5], while using SSD, we only need one shot to detect multiple images while R-CNN needs two shots, and that is the reason that SSD is much faster than R-CNN [6]. However, while detecting small-scale objects, SSD is much worse than RCNN since they can only be detected in higher resolution layers and SSD is slightly inaccurate than YOLO because the speed gets interrupted due to the gigantic model. Here is the image of the structure of the SSD [7]. SSD has two main components: multi-scale feature maps and convolutional predictor. Multi-scale feature maps improve the accuracy significantly because it uses multiple layers to detect objects independently, and that is one of the important features for this project. In facial mask detection, all images are not presented in the same scale, in that case, SSD makes it efficient for valid and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285f322-8f73-4caa-a194-fd57b9baf828",
   "metadata": {},
   "source": [
    "### 3.2.2 Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f8736-6514-48cb-8249-18f9c55b47fe",
   "metadata": {},
   "source": [
    "#### 3.2.2.1 SSD model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81b82d",
   "metadata": {},
   "source": [
    "SSD consists of two parts: the backbone model and the SSD head. In the figure below, the white boxes represent backbone and the last few blue boxes show the SSD head. The backbone model is usually a pre-trained image classification network as a feature extractor [7]. In this way, we get a deep neural network that is able to extract the semantics from the input image while preserving the spatial structure of the image. However, the resolution is extremely low. The SSD head is simply one or more convolutional layers added to the backbone, and the output is interpreted as a bounding box and object class in a spatial location activated by the final layer.\n",
    "\n",
    "The backbone provided by MMdetection consists of VGG-16 and mobilenetv2. Our group finally chose SSD-moblienetv2 as the deployed model; it is a single-stage target detection model, which is widely used for its compact network and novel depth separable volume. The one-stage detector requires only a single pass through the neural network and predicts all the bounding boxes in one go. SSD-moblienetv2 is a mobile friendly variant of regular SSD with high precision performance. Furthemore,  ssd-moblienetv2 reduce the parameter count and computational cost significantly compared to SSD, here is the table shows the comparison [8]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c54dc3-f79f-401d-8ea8-525beec92b36",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/ssd.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedd4c9-2ff1-433b-a388-168a23f55164",
   "metadata": {},
   "source": [
    "#### 3.2.2.2 MobileNetV2 model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ad033",
   "metadata": {},
   "source": [
    "MobileNetV2 is a convolutional neural network architecture, it is a single-stage target detection model, which is widely used for its compact network and novel depth separable volume. It is based on a reverse residual structure where the residual connections are located between the bottleneck layers. Mobilenetv2 architecture consists of an initial full convolution layer and 32 filters, followed by 19 remaining bottleneck layers.[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c04e3-026e-4144-916a-22d3cfd22008",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"350\" height = \"300\" src=\"./notebook_images/8.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dc77b-4ec7-4507-8690-8cb540c5e940",
   "metadata": {},
   "source": [
    "### 3.2.3 Experiment Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f07ac2-975e-4af2-8201-76e64482716a",
   "metadata": {},
   "source": [
    "We use MMDetection framework to train and test ssd-mobilenetv2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8fcb7-4ca6-40c5-a448-87e8d76f14bd",
   "metadata": {},
   "source": [
    "The source code of this part is in the directory `source_code/ssd-mobilenetv2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec0ca42-3861-4d66-a72f-39c8f216794c",
   "metadata": {},
   "source": [
    "#### Install environment\n",
    "- MMDetection works on Linux, Windows and macOS. It requires Python 3.6+, CUDA 9.2+ and PyTorch 1.5+.\n",
    "1. `conda create --name openmmlab python=3.8 -y`\n",
    "2. `conda activate openmmlab`\n",
    "3. `conda install pytorch=1.8 torchvision cudatoolkit=10.2 -c pytorch`\n",
    "( this step you should change according to your cuda version )\n",
    "4. `pip install -U openmim`\n",
    "5. `mim install mmcv-full`\n",
    "( MMDetection requires mmcv-full>=1.3.17, <1.7.0）\n",
    "6. `cd ssd-mobilenetv2`\n",
    "7. `pip install -r requirements/build.txt`\n",
    "8. `pip install -v -e .  # or \"python setup.py develop\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340a702-fff7-49bb-818a-4832b25340c7",
   "metadata": {},
   "source": [
    "#### Training\n",
    "We set most of hyperparameters as default.\n",
    "The hyperparameters of training can be found in [ssd_300epoch.log](experiment_result/ssd/ssd_300epoch.log) within the directory `experiment_result/ssd-mobilenetv2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e1e08-1109-4a93-80bc-7aa66c65fc60",
   "metadata": {},
   "source": [
    "We used the command below the run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15335885-fcb4-4540-85b9-0b161df633cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418a829-7cf3-4b5b-a133-f07e85425ac6",
   "metadata": {},
   "source": [
    "The train result is saved at the directory train_mask(when you train, it will create this new directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ed6a1-c07b-4308-9d0e-cd95c44bfb26",
   "metadata": {},
   "source": [
    "If train is interrupted, we used the command below the continue our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ee74b-1a5d-4c22-9f1a-321b62160d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./tools/train.py --resume-from ./train_mask/latest.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc802db-3503-4713-8560-30c734d10536",
   "metadata": {},
   "source": [
    "The training output can be found in this file: [ssd_result.json](experiment_result/ssd-mobilenetv2/ssd_result.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24ca06-588e-442c-b19d-16b7093033b6",
   "metadata": {},
   "source": [
    "### 3.2.4 Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4b180-0437-4236-af5d-4a3bf95c7edd",
   "metadata": {},
   "source": [
    "#### Confusion Matrix of SSD-mobilenetv2 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd52db-e7bc-44ab-a062-18f1586aee4a",
   "metadata": {},
   "source": [
    "The below figure is the confusion matrix of our best model of ssd-mobilenetv2. We can conclude that although the overall performance of the SSD-mobilenetv2 model is not bad in accuracy, especially when detecting a human face without a mask, the accuracy reached 73%. Nevertheless, the system will distinguish the background as human face or face with mask by mistakes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf427d8-7a4e-429f-8f1d-0e41e55e8794",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"250\" height = \"300\" src=\"experiment_result/ssd-mobilenetv2/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1ca2f-30ef-42db-b2de-f14954eaaec5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The plot of training for SSD-mobilenetv2 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210093a3-0c74-419c-8143-db259a88f070",
   "metadata": {},
   "source": [
    "Based on the the figure below we can see that the model is converged at about 200th epoch\n",
    "![jupyter](./notebook_images/ssd_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91d0e9-9348-4f84-89ed-24bb415ff69a",
   "metadata": {},
   "source": [
    "####  mAP of SSD-mobilenetv2 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982096a9-fbe6-457e-b4d9-4484e7c5b75a",
   "metadata": {},
   "source": [
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.69 |  0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a3b50-9061-4c2d-a935-a6fbd4c020fc",
   "metadata": {},
   "source": [
    "#### Inference Time of SSD-mobilenetv2 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8612a-876f-4bd3-9982-895966fab316",
   "metadata": {},
   "source": [
    "The inference time(FPS) is evaluate by the scrpit [benchmark.py](source_code/ssd-mobilenetv2/tools/analysis_tools/benchmark.py), the testing results varied sometimes, we chose the best performence as the final result, which was `85.6` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4a138-c6aa-4c07-a528-996f9115943f",
   "metadata": {},
   "source": [
    "## 3.3 Yolov5s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620fd58-edc6-4309-9296-6c3013d01900",
   "metadata": {},
   "source": [
    "### 3.3.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b555a-8c31-4c30-9e7b-d64dcb57adfe",
   "metadata": {},
   "source": [
    "YOLO is a state-of-the-art object detection system. It is one of the most popular algorithms for real-time object detection. YOLO(You Only Look Once) is a `single-stage detector`, which is the same as SSD but is different from RCNN.\n",
    "\n",
    "Since the first debut of YOLOv1 in 2016, YOLO family has been well developed and updated to YOLOv7 although Redmond, the author of YOLOv1, stopped working on YOLO after YOLOv3. For this project, our team chose to use YOLOv5, which was released in 2020, to perform facial mask detection. And instead of a single model, YOLOv5 is a family of compound-scaled object detection models[3], and `YOLOv5s` is used in our project.\n",
    "\n",
    "YOLOv5 comes with various versions, each having its own\n",
    "unique characteristic. These versions being:\n",
    "1. yolov5-n - The nano version\n",
    "2. yolov5-s - The small version\n",
    "3. yolov5-m - The medium version\n",
    "4. yolov5-l – The large version\n",
    "5. yolov5-x - The extra-large version\n",
    "\n",
    "The performance analysis of above models is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8e467-9ea9-4fd4-ba33-24717bacdbb4",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/4.png\" >\n",
    "\n",
    "\n",
    "refer:https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40ab26",
   "metadata": {},
   "source": [
    "The main process of object detection in YOLO can be demonstrated in the following figure. The system divides the “feature map” into S*S grid cells[11]. Each grid cell is responsible for detecting objects that belong to it. This is achieved by first generating three bounding boxes (including box coordinates and its confidence) and conditional class probability for each grid cell. Then, calculate the class probability for each bounding box by multiplying the confidence of the box with conditional class probability of the grid cell. Based on the class probability in the previous step, using NMS to eliminate redundant bounding boxes to get the final detections.\n",
    "![jupyter](./notebook_images/yolov1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18537a6-2245-4fec-9e98-290cbcf7063c",
   "metadata": {},
   "source": [
    "### 3.3.2 Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d13555-22b4-43c8-a1c7-12c0dbfb87ea",
   "metadata": {},
   "source": [
    "The YOLOv5 network consists of three main parts. \n",
    "1. Backbone - A CNN layer aggregate image features at\n",
    "different scales.\n",
    "2. Neck – Set of layers to combine image features and pass\n",
    "them forward to prediction.\n",
    "3. Head - Takes features from the neck and performs\n",
    "localization and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c86002-99a8-4e1a-90f6-74801961f519",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/5.png\" >\n",
    "ref: https://www.researchgate.net/figure/The-network-architecture-of-Yolov5-It-consists-of-three-parts-1-Backbone-CSPDarknet_fig1_349299852"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290e1dc-8de7-4d80-8e7f-a8584491f9c9",
   "metadata": {},
   "source": [
    "### 3.3.3 Experiment Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8e3ca-b033-45b8-a38a-0e1c1abb0d8f",
   "metadata": {},
   "source": [
    "The source code of this part is in the directory `source_code/yolov5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ec4aa-d468-4efc-b65c-b201271a5d06",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Install environment\n",
    "1. `cd source_code/yolov5`\n",
    "2. `conda create -n yolov5 python=3.8`\n",
    "3. `conda activate yolov5`\n",
    "4. `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0e82f-736e-417d-b177-14d23114dcb8",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55e0dd-7205-4185-af78-49e567671e24",
   "metadata": {},
   "source": [
    "We set most of hyperparameters as default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967d43c-4e18-4481-b3a8-a2ada85cfcd8",
   "metadata": {},
   "source": [
    "The hyperparameters of training can be found in [opt.yaml](experiment_result/yolov5s/640_5s_300epoch/opt.yaml) within the directory `experiment_result/yolov5s/640_5s_300epoch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335daace-3245-4cc7-a495-e71d37a0e504",
   "metadata": {},
   "source": [
    "We used the command below the run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b689e5-1153-45ed-92a2-5211cad41e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --epochs 300 --batch 64 --data './datasets/data.yaml' --cfg 'yolov5s.yaml' --weights '' --project 'facemask_640_yolo5s' --name '640_5s_300epoch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57147cf-8aec-4b87-9649-364f025dc3d9",
   "metadata": {},
   "source": [
    "The training output can be found in this file: [train_record.ipynb](source_code/yolov5/train_record.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8585c07-de87-4886-a3c2-40319e02abf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3.4 Results of YOLOv5s model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e039e-8799-4ca8-9d89-faa7ae2eea1d",
   "metadata": {},
   "source": [
    "#### Confusion Matrix of YOLOv5s model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483011f3-b7b1-42b6-b93a-beee13c70359",
   "metadata": {},
   "source": [
    "The below figure is the confusion matrix of our best model of yolov5s. Based on this graph, we can see that the background can be misdetected as face or mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21011d94-473c-4287-ac98-25a7bb969d9e",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"experiment_result/yolov5s/640_5s_300epoch/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bb4a1-d1d3-4b1b-85e6-cfba91ff2ef7",
   "metadata": {},
   "source": [
    "#### The plot of training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f9b45-2639-4480-95b8-5d41c828bad0",
   "metadata": {},
   "source": [
    "Based on the the figure below we can see that the model is converged at about 200th epoch \n",
    "![jupyter](./notebook_images/yolov5map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3ddca-ccd8-485e-ae9e-f3e30d659816",
   "metadata": {},
   "source": [
    "####  mAP of YOLOv5s model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e9902-5654-40f5-8f21-e137b2641acf",
   "metadata": {},
   "source": [
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.887 |  0.504   |\n",
    "| Face  | 0.931 |  0.557   |\n",
    "| Mask  | 0.843 |  0.451   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a18ff-20f9-49bd-9e95-28446aeca67b",
   "metadata": {},
   "source": [
    "We can see that the mask class much lower map than the face class, which matches the result of confusion, because the mask class are easier to be misdected as background than the face class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc10d90-9db9-4052-a170-1e18bf724f39",
   "metadata": {},
   "source": [
    "#### Inference Time of YOLOv5s model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbee02-1145-4344-9924-01f83cf2ff85",
   "metadata": {},
   "source": [
    "The inference time(FPS) is evaluate by the scrpit [val.py](source_code/yolov5/val.py), the testing results varied sometimes, we chose the best performence as the final result, which was `92.5` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7da38-fb52-45d0-b66a-c5b34cbf4152",
   "metadata": {},
   "source": [
    "## 3.4 Yolov5-MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d48af-7bf2-45d8-b8db-c56f9ee85704",
   "metadata": {},
   "source": [
    "### 3.4.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ed7e8-2028-434b-93c1-aaf40a0b8a37",
   "metadata": {},
   "source": [
    "We were inspired by the method of ssd-mobilenetV2, it integrates the SSD algorithm and MobileNetV2 networks. Therefore we decided to integrate yolov5 with moblieNet.\n",
    "\n",
    "The mobileNet has three versions, V1, V2 and V3. Its first version (mobileNetV1) has a depthwise separable convolution, which reduces the model complexity and computational cost. The key methmod used in the second version is called inverted residual structure, resulting in keeping the low computational cost and reaching relative higher accuracy compare with mobileNetV1. For V3, it combines the self-attention mechanism, and replaces relu activation with h-swish activation.[8]\n",
    "\n",
    "\n",
    "\n",
    "For comparision, we choose mobileNetV2, which is the same as the SSD model used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf38088-4b95-440e-bc88-a2a62b976e06",
   "metadata": {},
   "source": [
    "### 3.4.2 Custom Layers Modification in Yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a76fe-12d1-466f-9c53-10982dd6c258",
   "metadata": {},
   "source": [
    "As the part 3.3.3 introduced, yolov5 has three main parts, we only need to modify the backbone of the yolov5 model, which is used for extracting and processing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1aa3a5-b400-47da-8e9d-2d3a911959f0",
   "metadata": {},
   "source": [
    "ref: \n",
    "1. https://blog.csdn.net/weixin_42182534/article/details/123418604?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123418604-blog-114492726.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123418604-blog-114492726.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=2\n",
    "\n",
    "2. https://github.com/shaoshengsong/YOLOv5-ShuffleNetV2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257b3d5-4117-4956-9dd1-543d7424c2a4",
   "metadata": {},
   "source": [
    "To change the architecture, we need to do:\n",
    "\n",
    "1. add inverted residual structure in the yolov5 model module, see the file [common.py](source_code/yolov5-MobileNetV2/models/common.py), line no.876\n",
    "2. modify the model configuration file of original model, the key concept is to change the original CNN layers into invertedResidual layers, connecting with their corresponding head layers. The final configuration file can be found in the file [yolov5-moblienetv2.yaml](source_code/yolov5-MobileNetV2/models/yolov5-moblienetv2.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87ab49-d07a-44c8-a032-6222937dc7d3",
   "metadata": {},
   "source": [
    "The comparison of model complexity for Yolov5-MobileNetV2 and Yolov5s is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89223856-c9ba-4f8c-aee2-a6bb3ad5a942",
   "metadata": {},
   "source": [
    "|       Model        | Number of Parameters | Number of Layers |\n",
    "| :----------------: | :------------------: | :--------------: |\n",
    "|      Yolov5s       |       7015519        |       157        |\n",
    "| Yolov5-MobileNetV2 |       2916063        |       276        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b704c-aabd-46e0-87de-9d3596913341",
   "metadata": {},
   "source": [
    "### 3.4.3 Experiment Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae14a60-db9c-46cf-8a9c-c95ab87d1e06",
   "metadata": {},
   "source": [
    "The traning process and hyperparameter setting were the same as last experiment.\n",
    "\n",
    "The training output can be found in this file: [train_record.ipynb](source_code/yolov5-MobileNetV2/train_record.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590913f5-ec76-41f4-ab2e-e5a881991690",
   "metadata": {},
   "source": [
    "### 3.4.4 Results of YOLOv5s-mobilenetv2 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80d9ff-a2ec-4ef6-a35c-79dcd39c85e0",
   "metadata": {},
   "source": [
    "#### Confusion Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd21236-5007-4566-9c7b-e0159ce29e73",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"450\" height = \"300\" src=\"experiment_result/yolov5-MobileNetV2/640_300epoch/confusion_matrix.png\" >\n",
    "\n",
    "<img style=\"float: left;\" width = \"450\" height = \"300\" src=\"experiment_result/yolov5s/640_5s_300epoch/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5127b-36e1-4c0f-9ab2-f2300e7bb91b",
   "metadata": {},
   "source": [
    "According to the matrics above, the left is for the Yolov5-MoblieNetV2, the right is for the Yolov5s, we can found that the performance of accuracy slightly decreased. The characteristic of misdetection of mask,face and background remained as yolov5s model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fb258-11bb-4393-849e-b591ab3bb9c9",
   "metadata": {},
   "source": [
    "#### The plot of training:\n",
    "![jupyter](./notebook_images/yolov5map_mobilenetv2.png)\n",
    "Based on the the figure below we can see that the model is converged at about 100th epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908160e-1309-4d51-bff1-9d5329612cd4",
   "metadata": {},
   "source": [
    "####  mAP of YOLOv5s-mobilenetv2 model:\n",
    "\n",
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.871 |  0.478   |\n",
    "| Face  | 0.919 |  0.533   |\n",
    "| Mask  | 0.823 |  0.423   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b41dd-0205-4c53-82c6-46df5c7b3cf9",
   "metadata": {},
   "source": [
    "#### Inference Time of YOLOv5s-mobilenetv2 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8b53e-d1d9-46fb-ae4f-dcdd98f6435a",
   "metadata": {},
   "source": [
    "The testing results varied sometimes, we chose the best performence as the final result, which was `95.2` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3859dd-5897-440d-b1bc-5c27c6878a4d",
   "metadata": {},
   "source": [
    "# 4. Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21673a61",
   "metadata": {},
   "source": [
    "## 4.1 Comparision:\n",
    "Here’s the table concludes the result in each field:\n",
    "![jupyter](./notebook_images/compare_table.png)\n",
    "\n",
    "![jupyter](./notebook_images/4in1.png)\n",
    "In terms of mAP50, the original YOLOv5s model has the best mAP result of 88.7, while  Faster R-CNN can only achieve a mAP of 37.1. The result of YOLOv5s-mobilenetV2 is slightly lower than the original one. SSD-mobilenetV2 has a mAP of 68.9 which is between the result of Faster R-CNN and the result of YOLOv5s.\n",
    "\n",
    "In terms of inference speed, the YOLOv5s-mobilenetv2 has the best result of 95.2FPS, while Faster R-CNN can only achieve an inference speed of 48.1 FPS. The original YOLOv5s has a slower inference speed than the altered model. The performance of SSD-mobilenetV2 is between Faster R-CNN and YOLOv5s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5362ff4",
   "metadata": {},
   "source": [
    "## 4.2 Discussion:\n",
    "\n",
    "### 4.2.1 Data analysis\n",
    "As shown in the previous result table and figures, we can clearly see that  except for the Faster R-CNN model with mAP50 of 37.1 and inference speed of 48.1 FPS, all models we trained can achieve an acceptable performance on face mask detection job. \n",
    "Among them, the original YOLOv5s had the best result with mAP50 of 88.7 and inference speed of 92.5 FPS. \n",
    "As for the altered YOLOv5s model, after replacing the backbone of it, we managed to simplify the model by reducing the number of parameters from around 7m to 2.9m, leading to a smaller size and a faster inference speed from 92.5 FPS to 95.2 FPS with only a slight decrease in accuracy.\n",
    "As for the SSD model, it has a relatively poor performance than the YOLOv5s models with mAP of 68.9 and inference speed of 85.6.\n",
    "Overall, YOLOv5s can achieve a better performance compared with Faster R-CNN and SSD. After replacing the complex backbone network to a relatively lightweight network, such as mobilenetv2, YOLOv5s can achieve an even faster inference speed. After the comparison among all models we tried, we believe the YOLOv5s-mobilenetv2 model meets the goal of our project to find a real time face mask detection model with good trade-off.\n",
    "\n",
    "\n",
    "### 4.2.2 Error analysis\n",
    "While deploying the YOLOv5 model, based on the mAP value we got, our group found that the mask class had much lower mAP value than the face class, which matches the result of confusion, because the mask class is easier to be misdetected as background than the face class.\n",
    "\n",
    "To solve this problem, our group prepared to add extra training images which contained neither a human face nor human face with a mask to train our model. Further experiments might be included in possible future work.\n",
    "\n",
    "\n",
    "### 4.2.3 Possible future work\n",
    "As future work, we will conduct further experiments to evaluate the performance of the proposed solutions. For example, our group chose the YOLO version 5s as one of the deployed models since it was released in 2020 and is the most popular architecture among yolo families. Yolov5 has other models, 5n, 5m,5l, and 5x. The complexity of these models increases gradually. It will cost more inference time if a model is more complex than others, due to the limitation of time in this project, we may try another version of the YOLO model in the future to test if there is any chance to improve the performance of the system. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda29a2d",
   "metadata": {},
   "source": [
    "# 5.Reference\n",
    "\n",
    "[1] Jatin Prakash, “Non Maximum Suppression: Theory and Implementation in PyTorch”, [Online]. Available: \n",
    "https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/\n",
    "[Accessed: 04-Nov-2022]\n",
    "\n",
    "[2] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” arXiv.org, 22-Oct-2014. [Online]. Available: https://arxiv.org/abs/1311.2524. [Accessed: 03-Nov-2022]. \n",
    "\n",
    "[3] R. Girshick, “Fast R-CNN,” arXiv.org, 27-Sep-2015. [Online]. Available: https://arxiv.org/abs/1504.08083. [Accessed: 08-Nov-2022]. \n",
    "\n",
    "[4] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time object detection with region proposal networks,” arXiv.org, 06-Jan-2016. [Online]. Available: https://arxiv.org/abs/1506.01497. [Accessed: 08-Nov-2022]. \n",
    "\n",
    "[5] Renu Khandelwal, “SSD : Single Shot Detector for object detection using MultiBox\n",
    "”, Dec 1, 2019, [Online]. Available: https://towardsdatascience.com/ssd-single-shot-detector-for-object-detection-using-multibox-1818603644ca\n",
    "[Accessed: 30-Oct-2022]\n",
    "\n",
    "[6] Jonathan Hui, “SSD object detection: Single Shot MultiBox Detector for real-time processing\n",
    "”, Mar 14, 2018, [Online]. Available:https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06\n",
    "[Accessed: 30-Oct-2022]\n",
    "\n",
    "[7] Esri, “How single-shot detector (SSD) works?”, [Online]. Available: \n",
    "https://developers.arcgis.com/python/guide/how-ssd-works/\n",
    "[Accessed: 31-Oct-2022]\n",
    "\n",
    "[8] Mark Sandler, “MobileNetV2: Inverted Residuals and Linear Bottlenecks”, https://arxiv.org/pdf/1801.04381.pdf [Accessed: 11-Nov-2022]\n",
    "\n",
    "[9] Sandler et al., “MobileNetV2”, https://paperswithcode.com/method/mobilenetv2 [Accessed: 19-Nov-2022]\n",
    "\n",
    "[10] Ultralytics, “ultralytics_yolov5”, [Online]. Available: \n",
    "https://pytorch.org/hub/ultralytics_yolov5/\n",
    "[Accessed: 04-Nov-2022]\n",
    "\n",
    "[11] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You Only Look Once: Unified, Real-Time Object Detection” arXiv.org, 9-May-2016. [Online]. Available:https://arxiv.org/abs/1506.02640. [Accessed: 03-Nov-2022]. \n",
    "\n",
    "[12] A. Bochkovskiy, C. Wang and H. Liao, “YOLOv4: Optimal Speed and Accuracy of Object Detection” arXiv.org, 23-Apr-2022. [Online]. Available: https://arxiv.org/abs/2004.10934. [Accessed: 04-Nov-2022]. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
