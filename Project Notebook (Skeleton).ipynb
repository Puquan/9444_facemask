{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cc9da9-ebb2-438c-8da8-34a73b16e4d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FaceMask Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e990bc1-3fd6-4785-936a-d5e633553eb4",
   "metadata": {},
   "source": [
    "`Group Name: Gogogo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755c7b7-e9dc-4aba-a9e9-145127d0ceac",
   "metadata": {},
   "source": [
    "\n",
    "|     Name      | Student ID |\n",
    "| :-----------: | :--------: |\n",
    "|  Puquan Chen  |  z5405329  |\n",
    "| Wenzhen Zhang |  z5282188  |\n",
    "|   Zeran Qiu   |  z5237346  |\n",
    "| Xiaolan Zhang |  z5400028  |\n",
    "|  Haoyu Zang   |  z5326339  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da956912-10aa-46b7-91fa-909b7d0f5074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3c167d3-1290-449a-be41-e5726dce16a1",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c305c-db80-414e-944d-356e29879ac8",
   "metadata": {},
   "source": [
    "## 1.1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b85cc-4338-462f-9a9b-bf255c93853b",
   "metadata": {},
   "source": [
    "COVID-19 has been going on for three years. Although its impact is waning in many areas, we still do not have a good solution to the harm it causes to humans. Therefore the use of facial masks has significantly increased on many occasions. Nevertheless, it is still important to develop social awareness of residents to wear facial masks, since it is common to find people in narrow public spaces without masks. In this case, we need face mask detection to help local authorities better monitor people's compliance with local mask wearing policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b36fc0-71fc-4eff-8ef7-1db832135a87",
   "metadata": {},
   "source": [
    "## 1.2 Motivation\n",
    "As mentioned above, in a climate where COVID-19 is still spreading, we need to ensure that people comply with the mask wearing policy. However, it is very difficult to identify whether people are wearing a mask by human intervention alone, especially in crowded situations such as trains and shopping malls. Therefore, there is a great need for face mask detection to help us quickly and accurately identify whether people are wearing masks or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b35c9-d621-4775-91b2-c4ea5fbbadfb",
   "metadata": {},
   "source": [
    "## 1.3 Purpose/ Project Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3a69e-2768-43d0-9d93-77675a6af80f",
   "metadata": {},
   "source": [
    "\n",
    "In this project, our group applied three existing popular deep learning object detection models which includes `RCNN`, `YOLO` and `SSD`, aim to compare their performance in accuracy, training time and loss rate by using the same data set. Aftering getting all necessary data and details, our group will build our own model based on the existing models mentioned in above and try to analysis the methods which can be used and improve the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3844475-7d00-4afe-8416-a1f5e47ac501",
   "metadata": {},
   "source": [
    "# 2. Data Sources / Preparation of Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685d080-4022-4398-84de-2d07bbd7a186",
   "metadata": {},
   "source": [
    "**All scripts used in this part are stored in the directory** `./tool_scripts` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cb077-1ff1-4e64-a62f-5a0f08bc51f2",
   "metadata": {},
   "source": [
    "## 2.1 Basic Information about The Data \n",
    "\n",
    "Our original face mask dataset contains 6120 images, it comes from https://github.com/AIZOOTech/FaceMaskDetection\n",
    "\n",
    "Nevertheless, the Face Mask Dataset is a combined dataset, the training set made up of 3114 images of Wider Face and 3006 images of masking FAces (MAFA) datasets. It contains normal faces with different lighting, poses, occlusion, and masked face.The test set has 1839 images which includes 780 Wider Face and 1059 MAFA dataset. Here are some samples of original images include in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266dba74-fa12-4802-ab01-8245ceb11018",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 这边泽然补一下sample image，然后看看这部分上下，语句有没有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32ace4-f605-4e0c-8084-6ac4a5b7a00e",
   "metadata": {},
   "source": [
    "Since some format of label files in original dataset are not readable, the pre-processing has been applied to the original dataset. The images in available formats were identified and some invalid dataset was deleted. However, after processing and editing, the number of reliable images reduced dramatically which could affect the following training, our group collected and added extra images to ensure the amount and quality of the whole dataset. Extra datasets refers to the link below:\n",
    "1. https://public.roboflow.com/object-detection/mask-wearing\n",
    "2. https://www.kaggle.com/datasets/andrewmvd/face-mask-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbfecb-4352-4280-af32-24a1ac70f01c",
   "metadata": {},
   "source": [
    "## 2.2 Split Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66033d95-2645-4d19-90da-1277c6797f2c",
   "metadata": {},
   "source": [
    "After integrating three datasets, we try to split the whole datasets into training set, validation set and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221497b-cc70-4787-b527-5d023cce5501",
   "metadata": {},
   "source": [
    "We used [split_dataset.py](./tool_scripts/split_dataset.py) to do the spliting task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c038db-5e74-40f9-b987-8cda5a38af70",
   "metadata": {},
   "source": [
    "The script can be run on dataset with the command below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9081b-cc37-46e8-a6e6-241f5ccb2bd7",
   "metadata": {},
   "source": [
    "`python3 split_dataset.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc64c1-2def-441d-ab82-f14259fca502",
   "metadata": {},
   "source": [
    "Our final face mask dataset contains `18756` training images, `2993` validation images and `2993` testing images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec5e0b-30c9-4778-818e-603547987d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Process Dataset Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12ac7c-9ef6-4ab6-b2ba-10851e1fa660",
   "metadata": {},
   "source": [
    "We tried to train our models based on different frameworks, we used `yolov5` for yolo and `mmdetection` for others, but they need different format of labels, such as `xml`, `txt`, and `json`. Therefore, we created and ran scripts that those label files can be interchangeable\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beaf857-e820-4781-ac9c-5cc47c27f770",
   "metadata": {},
   "source": [
    "The usage of those scripts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c564-8dc6-415b-931a-3faecb326d94",
   "metadata": {},
   "source": [
    "- 1. [formatting_xml.py](./tool_scripts): reformat the `xml` files so that it can be processed by other scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2938ca-c42b-4c0c-9a6c-a34338b0a171",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223e7c9-3e87-466a-ad62-66ba9befda31",
   "metadata": {},
   "source": [
    "- 2. [xml_to_txt.py](./tool_scripts/xml_to_txt.py): convert `xml` format labels to `txt` format labels. The `txt` format is also called yolo format, which only contains the information about object class, object coordinates, height, and width as the image shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f92f3-20bd-4ef0-9927-cab0521afd6b",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70756f56-c48b-4e88-ad4c-ac985eaa7dd1",
   "metadata": {},
   "source": [
    "- 3. [txt_to_xml.py](./tool_scripts/txt_to_xml.py): convert `txt` format labels to `xml` format labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807cf088-2a6a-46d8-b303-97f00ebc2779",
   "metadata": {},
   "source": [
    "Code reference of this part can be found in [tool_scripts/README.md](./tool_scripts/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb146e9-1143-4215-abe0-ad7e0d97d57a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8208c2d-c319-4c09-9a74-b5a685b444b0",
   "metadata": {},
   "source": [
    "The source codes of each part of experiments are stored in the directory `source_code`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602fd6d8-659f-4a9e-959c-d008d3d663aa",
   "metadata": {},
   "source": [
    "All models were trained and tested on a [Cloud platform](https://www.autodl.com/), using Nvidia RTX 3090. (GPU Memory Size: 24 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715533a-bc44-4d15-b225-2b882ee0e6c8",
   "metadata": {},
   "source": [
    "We tried to stop training until each model became converged, but it was very time-comsuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920f3f9-c0ec-4a25-b2e5-60c6d4ebf150",
   "metadata": {},
   "source": [
    "## 3.1 RCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a511534-87a0-48fd-b09d-3fb48fe72ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc59a234-014c-4d86-8d3c-4084367c0c4f",
   "metadata": {},
   "source": [
    "## 3.2 SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a720c-7276-406e-978d-210f46228c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8b4a138-c6aa-4c07-a528-996f9115943f",
   "metadata": {},
   "source": [
    "## 3.3 Yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620fd58-edc6-4309-9296-6c3013d01900",
   "metadata": {},
   "source": [
    "### 3.3.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b555a-8c31-4c30-9e7b-d64dcb57adfe",
   "metadata": {},
   "source": [
    "YOLO is a state-of-the-art object detection system. It is one of the most popular algorithms for real-time object detection. YOLO(You Only Look Once) is a `single-stage detector`, which is the same as SSD but is different from RCNN.\n",
    "\n",
    "Since the first debut of YOLOv1 in 2016, YOLO family has been well developed and updated to YOLOv7 although Redmond, the author of YOLOv1, stopped working on YOLO after YOLOv3. For this project, our team chose to use YOLOv5, which was released in 2020, to perform facial mask detection. And instead of a single model, YOLOv5 is a family of compound-scaled object detection models[3], and `YOLOv5s` is used in our project.\n",
    "\n",
    "YOLOv5 comes with various versions, each having its own\n",
    "unique characteristic. These versions being:\n",
    "1. yolov5-n - The nano version\n",
    "2. yolov5-s - The small version\n",
    "3. yolov5-m - The medium version\n",
    "4. yolov5-l – The large version\n",
    "5. yolov5-x - The extra-large version\n",
    "\n",
    "The performance analysis of above models is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8e467-9ea9-4fd4-ba33-24717bacdbb4",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/4.png\" >\n",
    "\n",
    "\n",
    "refer:https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a325b88-fd98-4745-9f1a-f31e5bc044a7",
   "metadata": {},
   "source": [
    "### 3.3.2 Key Terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f12bc-5bbe-4aa7-b4ad-548a8047c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 这部分文朕把自己report里面的东西加进来，图片啥的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f89fe-0ed3-4253-8e2f-08e51acb113e",
   "metadata": {},
   "source": [
    "The main process of object detection in YOLO can be demonstrated in the following figure. The system divides the “feature map” into S*S grid cells[4]. Each grid cell is responsible for detecting objects that belong to it. This is achieved by first generating three bounding boxes (including box coordinates and its confidence) and conditional class probability for each grid cell. Then, calculate the class probability for each bounding box by multiplying the confidence of the box with conditional class probability of the grid cell. Based on the class probability in the previous step, using NMS to eliminate redundant bounding boxes to get the final detections.\n",
    "\n",
    "Figure below (cited from yolov4 paper)[5] demonstrates the structure of one stage detector (e.g., YOLO) and two stage detector (e.g., Faster R-CNN)\n",
    "\n",
    "Anchor box: Anchor boxes are a set of predefined boxes with scaled sizes and width-height ratios. Anchor box was first adopted in YOLO since YOLOv2. The main idea of using anchor boxes is, instead of generating the entire bounding box, bounding boxes for each grid cell can be created by tuning the initial anchor boxes. And the size and shape of anchor boxes can be set up either with experience or using k-means clusters on the ground truth boxes of the dataset. In YOLOv5, other than setting up the anchor by hand, the system could adaptively calculate the best anchor based on the dataset we use during the training stage.\n",
    "Intersection over Union (IoU): The Intersection over Union (IoU) metric is essentially a method used usually to quantify the percent overlap between the ground truth BBox (Bounding Box) and the prediction BBox. However, in NMS, we find IoU between two predictions BBoxes instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18537a6-2245-4fec-9e98-290cbcf7063c",
   "metadata": {},
   "source": [
    "### 3.3.3 Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d13555-22b4-43c8-a1c7-12c0dbfb87ea",
   "metadata": {},
   "source": [
    "The YOLOv5 network consists of three main parts. \n",
    "1. Backbone - A CNN layer aggregate image features at\n",
    "different scales.\n",
    "2. Neck – Set of layers to combine image features and pass\n",
    "them forward to prediction.\n",
    "3. Head - Takes features from the neck and performs\n",
    "localization and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c86002-99a8-4e1a-90f6-74801961f519",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"./notebook_images/5.png\" >\n",
    "ref: https://www.researchgate.net/figure/The-network-architecture-of-Yolov5-It-consists-of-three-parts-1-Backbone-CSPDarknet_fig1_349299852"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290e1dc-8de7-4d80-8e7f-a8584491f9c9",
   "metadata": {},
   "source": [
    "### 3.3.4 Experiment Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8e3ca-b033-45b8-a38a-0e1c1abb0d8f",
   "metadata": {},
   "source": [
    "The source code of this part is in the directory `source_code/yolov5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ec4aa-d468-4efc-b65c-b201271a5d06",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Install environment\n",
    "1. `cd source_code/yolov5`\n",
    "2. `conda create -n yolov5 python=3.8`\n",
    "3. `conda activate yolov5`\n",
    "4. `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0e82f-736e-417d-b177-14d23114dcb8",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55e0dd-7205-4185-af78-49e567671e24",
   "metadata": {},
   "source": [
    "We set most of hyperparameters as default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967d43c-4e18-4481-b3a8-a2ada85cfcd8",
   "metadata": {},
   "source": [
    "The hyperparameters of training can be found in [opt.yaml](experiment_result/yolov5s/640_5s_300epoch/opt.yaml) within the directory `experiment_result/yolov5s/640_5s_300epoch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335daace-3245-4cc7-a495-e71d37a0e504",
   "metadata": {},
   "source": [
    "We used the command below the run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b689e5-1153-45ed-92a2-5211cad41e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --epochs 300 --batch 64 --data './datasets/data.yaml' --cfg 'yolov5s.yaml' --weights '' --project 'facemask_640_yolo5s' --name '640_5s_300epoch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57147cf-8aec-4b87-9649-364f025dc3d9",
   "metadata": {},
   "source": [
    "The training output can be found in this file: [train_record.ipynb](source_code/yolov5/train_record.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8585c07-de87-4886-a3c2-40319e02abf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4.5 Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e039e-8799-4ca8-9d89-faa7ae2eea1d",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483011f3-b7b1-42b6-b93a-beee13c70359",
   "metadata": {},
   "source": [
    "The below figure is the confusion matrix of our best model of yolov5s. Based on this graph, we can see that the background can be misdetected as face or mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21011d94-473c-4287-ac98-25a7bb969d9e",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"650\" height = \"300\" src=\"experiment_result/yolov5s/640_5s_300epoch/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bb4a1-d1d3-4b1b-85e6-cfba91ff2ef7",
   "metadata": {},
   "source": [
    "#### The Plot of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f9b45-2639-4480-95b8-5d41c828bad0",
   "metadata": {},
   "source": [
    "Based on the the figure below we can see that the model is converged at about 200th epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be558be-20b8-4f55-9ffa-a3db0098b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 图, 就画一个map的图，x轴的epoch需要显示的比较明显一些，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3ddca-ccd8-485e-ae9e-f3e30d659816",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e9902-5654-40f5-8f21-e137b2641acf",
   "metadata": {},
   "source": [
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.887 |  0.504   |\n",
    "| Face  | 0.931 |  0.557   |\n",
    "| Mask  | 0.843 |  0.451   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a18ff-20f9-49bd-9e95-28446aeca67b",
   "metadata": {},
   "source": [
    "We can see that the mask class much lower map than the face class, which matches the result of confusion, because the mask class are easier to be misdected as background than the face class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc10d90-9db9-4052-a170-1e18bf724f39",
   "metadata": {},
   "source": [
    "#### Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbee02-1145-4344-9924-01f83cf2ff85",
   "metadata": {},
   "source": [
    "The inference time(FPS) is evaluate by the scrpit [val.py](source_code/yolov5/val.py), the testing results varied sometimes, we chose the best performence as the final result, which was `92.5` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7da38-fb52-45d0-b66a-c5b34cbf4152",
   "metadata": {},
   "source": [
    "## 3.4 Yolov5-MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d48af-7bf2-45d8-b8db-c56f9ee85704",
   "metadata": {},
   "source": [
    "### 3.4.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ed7e8-2028-434b-93c1-aaf40a0b8a37",
   "metadata": {},
   "source": [
    "We were inspired by the method of ssd-mobilenetV2, it integrates the SSD algorithm and MobileNetV2 networks. Therefore we decided to integrate yolov5 with moblieNet.\n",
    "\n",
    "The mobileNet has three versions, V1, V2 and V3. Its first version (mobileNetV1) has a depthwise separable convolution, which reduces the model complexity and computational cost. The key methmod used in the second version is called inverted residual structure, resulting in keeping the low computational cost and reaching relative higher accuracy compare with mobileNetV1. For V3, it combines the self-attention mechanism, and replaces relu activation with h-swish activation.[1]这里加reference，mobilenet的论文\n",
    "\n",
    "\n",
    "\n",
    "For comparision, we choose mobileNetV2, which is the same as the SSD model used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf38088-4b95-440e-bc88-a2a62b976e06",
   "metadata": {},
   "source": [
    "### 3.4.2 Custom Layers Modification in Yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a76fe-12d1-466f-9c53-10982dd6c258",
   "metadata": {},
   "source": [
    "As the part 3.3.3 introduced, yolov5 has three main parts, we only need to modify the backbone of the yolov5 model, which is used for extracting and processing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1aa3a5-b400-47da-8e9d-2d3a911959f0",
   "metadata": {},
   "source": [
    "ref: \n",
    "1. https://blog.csdn.net/weixin_42182534/article/details/123418604?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123418604-blog-114492726.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123418604-blog-114492726.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=2\n",
    "\n",
    "2. https://github.com/shaoshengsong/YOLOv5-ShuffleNetV2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257b3d5-4117-4956-9dd1-543d7424c2a4",
   "metadata": {},
   "source": [
    "To change the architecture, we need to do:\n",
    "\n",
    "1. add inverted residual structure in the yolov5 model module, \n",
    "2. modify the model configuration file of original model, the key concept is to change the original CNN layers into invertedResidual layers, connecting with their corresponding head layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b704c-aabd-46e0-87de-9d3596913341",
   "metadata": {},
   "source": [
    "### 3.4.3 Experiment Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae14a60-db9c-46cf-8a9c-c95ab87d1e06",
   "metadata": {},
   "source": [
    "The traning process and hyperparameter setting were the same as last experiment.\n",
    "\n",
    "The training output can be found in this file: [train_record.ipynb](source_code/yolov5-MobileNetV2/train_record.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590913f5-ec76-41f4-ab2e-e5a881991690",
   "metadata": {},
   "source": [
    "### 3.4.4 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80d9ff-a2ec-4ef6-a35c-79dcd39c85e0",
   "metadata": {},
   "source": [
    "#### Confusion Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd21236-5007-4566-9c7b-e0159ce29e73",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" width = \"450\" height = \"300\" src=\"experiment_result/yolov5-MobileNetV2/640_300epoch/confusion_matrix.png\" >\n",
    "\n",
    "<img style=\"float: left;\" width = \"450\" height = \"300\" src=\"experiment_result/yolov5s/640_5s_300epoch/confusion_matrix.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5127b-36e1-4c0f-9ab2-f2300e7bb91b",
   "metadata": {},
   "source": [
    "According to the matrics above, the left is for the Yolov5-MoblieNetV2, the right is for the Yolov5s, we can found that the performance of accuracy slightly decreased. The characteristic of misdetection of mask,face and background remained as yolov5s model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fb258-11bb-4393-849e-b591ab3bb9c9",
   "metadata": {},
   "source": [
    "#### The Plot of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc87f6-ecbc-40e0-b985-0f1a0d4d9c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7f5c873-c51b-4617-b248-cb55698f6c9d",
   "metadata": {},
   "source": [
    "Based on the the figure below we can see that the model is converged at about 100th epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac510f-a60d-400e-821a-c183e220aceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9908160e-1309-4d51-bff1-9d5329612cd4",
   "metadata": {},
   "source": [
    "#### Map\n",
    "\n",
    "The table below shows the map value of our model.\n",
    "\n",
    "| Class | map50 | map50-95 |\n",
    "| :---: | :---: | :------: |\n",
    "|  All  | 0.871 |  0.478   |\n",
    "| Face  | 0.919 |  0.533   |\n",
    "| Mask  | 0.823 |  0.423   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b41dd-0205-4c53-82c6-46df5c7b3cf9",
   "metadata": {},
   "source": [
    "#### Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8b53e-d1d9-46fb-ae4f-dcdd98f6435a",
   "metadata": {},
   "source": [
    "The testing results varied sometimes, we chose the best performence as the final result, which was `95.2` fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3859dd-5897-440d-b1bc-5c27c6878a4d",
   "metadata": {},
   "source": [
    "# 4. Comparision/ Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a78141-2fcc-493b-8516-841ff32801f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
